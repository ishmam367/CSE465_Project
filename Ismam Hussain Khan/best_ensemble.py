# -*- coding: utf-8 -*-
"""Untitled21.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1EmaNBS5T1IpUugnIkCHNGkVd7yFXr59l
"""

import numpy as np 
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier
from sklearn. ensemble import RandomForestClassifier, BaggingClassifier, AdaBoostClassifier, VotingClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC

dataset=pd.read_csv('liver.csv')
dataset.isnull().sum()
dataset['Albumin_and_Globulin_Ratio'].mean()
dataset=dataset.fillna(0.94)
dataset.head()

X=dataset[['Age', 'Total_Bilirubin', 'Direct_Bilirubin',
       'Alkaline_Phosphotase', 'Alamine_Aminotransferase',
       'Aspartate_Aminotransferase', 'Total_Protiens', 'Albumin',
       'Albumin_and_Globulin_Ratio']]
y=dataset['Dataset']
X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.3,random_state=123)

dt = DecisionTreeClassifier()
dt.fit(X_train,y_train)
dt.score(X_train,y_train)
dt.score(X_test,y_test)

#Bagging for reducing variance so it doesnt overfit (max sample 0.1 means sample size is 10%)

bg = BaggingClassifier(DecisionTreeClassifier(), max_samples= 0.1, max_features = 1.0, n_estimators = 30)
bg.fit(X_train,y_train)
bg.score(X_train,y_train)

bg.score(X_test,y_test)

#Boosting - Ada Boost

adb = AdaBoostClassifier(DecisionTreeClassifier(),n_estimators = 10, learning_rate = 0.1)
adb.fit(X_train,y_train)

adb.score(X_train,y_train)

adb.score(X_test,y_test)

lr = LogisticRegression()
dt = DecisionTreeClassifier()
svm = SVC(kernel = 'poly', degree = 2 )#try sigmoid ,linear kernel too.
lr.fit(X_train,y_train)
print("logistic regresson  training score is ",lr.score(X_train,y_train))
print("logistic regresson  testing score is ",lr.score(X_test,y_test))

svm.fit(X_train,y_train)
print("svm  training score is ",svm.score(X_train,y_train))
print("svm  testing score is ",svm.score(X_test,y_test))
dt.fit(X_train,y_train)
print("DT  training score is ",dt.score(X_train,y_train))
print("DT  testing score is ",dt.score(X_test,y_test))

from sklearn.neighbors import KNeighborsClassifier
KNN = KNeighborsClassifier(n_neighbors=10,p=1)
KNN.fit(X_train, y_train)
knn_predicted=KNN.predict(X_test)
knn_score_train = round(KNN.score(X_train, y_train) * 100, 2)
knn_score_test = round(KNN.score(X_test, y_test) * 100, 2)
print('KNN Training Score: \n', knn_score_train)
print('KNN Test Score: \n', knn_score_test)

from sklearn.ensemble import RandomForestClassifier
random_forest = RandomForestClassifier(n_estimators=100)
random_forest.fit(X_train, y_train)
rf_predicted = random_forest.predict(X_test)
random_forest_score_train = round(random_forest.score(X_train, y_train) * 100, 2)
random_forest_score_test = round(random_forest.score(X_test, y_test) * 100, 2)

print('Random Forest Training Score: \n', random_forest_score_train)
print('Random Forest Test Score: \n', random_forest_score_test)

# Voting Classifier - Multiple Model Ensemble 

evc = VotingClassifier( estimators= [('lr',lr),('svm',svm),('bg',bg),('KNN',KNN),('random_forest',random_forest)], voting = 'hard')
evc.fit(X_train,y_train)
print("ensemble  training score is ",evc.score(X_train,y_train))
print("ensemble  testing score is ",evc.score(X_test,y_test))