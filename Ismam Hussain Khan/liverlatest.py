# -*- coding: utf-8 -*-
"""liverlatest.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1nvSthbSRd94_YHrijrKswGenbz6T1fiP
"""

import pandas as pd
from sklearn.model_selection import StratifiedShuffleSplit
from sklearn.ensemble import RandomForestClassifier, VotingClassifier, BaggingClassifier
from sklearn.svm import SVC
from sklearn.dummy import DummyClassifier
from sklearn.naive_bayes import MultinomialNB
from sklearn.linear_model import LogisticRegressionCV
from sklearn.neighbors import KNeighborsClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn import metrics 
from sklearn.model_selection import cross_val_score, cross_val_predict

liv_df = pd.read_csv('liver.csv')
liv_df.head()

liv_df['Albumin_and_Globulin_Ratio'].mean()
liv_df=liv_df.fillna(0.94)

from sklearn.model_selection import train_test_split
liv_df['Gender']=liv_df['Gender'].apply(lambda x:1 if x=='Male' else 0)
X=liv_df[['Age','Gender','Total_Bilirubin', 'Direct_Bilirubin',
         'Alkaline_Phosphotase', 'Alamine_Aminotransferase',
        'Aspartate_Aminotransferase', 'Total_Protiens', 'Albumin',
        'Albumin_and_Globulin_Ratio']]
y = liv_df['Dataset']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 123)

random_forest = RandomForestClassifier(n_estimators=100)
random_forest.fit(X_train, y_train)
rf_predicted = random_forest.predict(X_test)
random_forest_score_train = round(random_forest.score(X_train, y_train) * 100, 2)
random_forest_score_test = round(random_forest.score(X_test, y_test) * 100, 2)

print('Random Forest Training Score: \n', random_forest_score_train)
print('Random Forest Test Score: \n', random_forest_score_test)

from sklearn.metrics import classification_report, confusion_matrix
cmatrf = confusion_matrix(y_test,rf_predicted)
print('Confusion matrix RF: \n',cmatrf)
crRf= classification_report(y_test,rf_predicted)
print('Classification report RF: \n',crRf)

svclassifier = SVC(kernel='linear')
svclassifier.fit(X_train, y_train)
svm_predicted = svclassifier.predict(X_test)
svc_score_train = round(svclassifier.score(X_train, y_train) * 100, 2)
svc_score_test = round(svclassifier.score(X_test, y_test) * 100, 2)
print('SVM Training Score: \n', svc_score_train)
print('SVM Test Score: \n', svc_score_test)

cmatsvm = confusion_matrix(y_test,svm_predicted)
print('Confusion matrix SVM: \n',cmatsvm)
crsvm= classification_report(y_test,rf_predicted)
print('Classification report SVM: \n',crsvm)

KNN = KNeighborsClassifier(n_neighbors=10,p=1)
KNN.fit(X_train, y_train)
knn_predicted=KNN.predict(X_test)
knn_score_train = round(KNN.score(X_train, y_train) * 100, 2)
knn_score_test = round(KNN.score(X_test, y_test) * 100, 2)
print('KNN Training Score: \n', knn_score_train)
print('KNN Test Score: \n', knn_score_test)
cmatknn = confusion_matrix(y_test,knn_predicted)
print('Confusion matrix knn: \n',cmatsvm)
crsvm= classification_report(y_test,knn_predicted)
print('Classification report knn: \n',crsvm)

from sklearn.linear_model import LogisticRegression
logreg = LogisticRegression()
# Train the model using the training sets and check score
logreg.fit(X_train, y_train)
#Predict Output
log_predicted= logreg.predict(X_test)

logreg_score_train = round(logreg.score(X_train, y_train) * 100, 2)
logreg_score_test = round(logreg.score(X_test, y_test) * 100, 2)
#Equation coefficient and Intercept
print('Logistic Regression Training Score: \n', logreg_score_train)
print('Logistic Regression Test Score: \n', logreg_score_test)
cmatlr = confusion_matrix(y_test,log_predicted)
print('Confusion matrix knn: \n',cmatlr)
crlr= classification_report(y_test,log_predicted)
print('Classification report knn: \n',crlr)

bg = BaggingClassifier(DecisionTreeClassifier(), max_samples= 0.1, max_features = 1.0, n_estimators = 30)
bg.fit(X_train,y_train)
bg.score(X_train,y_train)

bg.score(X_test,y_test)

evc = VotingClassifier( estimators= [('logreg',logreg),('svclassifier',svclassifier),('bg',bg),('KNN',KNN),('random_forest',random_forest)], voting = 'hard')
evc.fit(X_train,y_train)
print("ensemble  training score is ",evc.score(X_train,y_train))
print("ensemble  testing score is ",evc.score(X_test,y_test))
ens_predicted= evc.predict(X_test)
cmatens = confusion_matrix(y_test,ens_predicted)
print('Confusion matrix ensemble: \n',cmatens)
crens= classification_report(y_test,ens_predicted)
print('Classification report ensemble: \n',crens)

#cross validation
from sklearn.model_selection import cross_val_score

ENSscore=cross_val_score(evc, X_train, y_train, cv=10)#, scoring='f1_micro')
LRscore=cross_val_score(logreg, X_train, y_train, cv=10)#, scoring='f1_micro')
KNNscore=cross_val_score(KNN, X_train, y_train, cv=10)#, scoring='f1_micro')

RFscore=cross_val_score(random_forest, X_train, y_train, cv=10)#, scoring='f1_micro')
SVMscore=cross_val_score(svclassifier, X_train, y_train, cv=10)#, scoring='f1_micro')

print("Cross-Validation: \n")
print("Ensemble Accuracy: %0.2f (+/- %0.2f)" % (ENSscore.mean(), ENSscore.std() * 2))
print("LR Accuracy: %0.2f (+/- %0.2f)" % (LRscore.mean(), LRscore.std() * 2))
print("KNN Accuracy: %0.2f (+/- %0.2f)" % (KNNscore.mean(), KNNscore.std() * 2))
print("RF Accuracy: %0.2f (+/- %0.2f)" % (RFscore.mean(), RFscore.std() * 2))
print("SVM Accuracy: %0.2f (+/- %0.2f)" % (SVMscore.mean(), SVMscore.std() * 2))
print()